{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c918b7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sport            191\n",
      "business         187\n",
      "tech             152\n",
      "entertainment    140\n",
      "politics         129\n",
      "Name: Category, dtype: int64\n",
      "   ArticleId                                               Text  Category  \\\n",
      "0       1833  worldcom ex-boss launches defence lawyers defe...  business   \n",
      "1        154  german business confidence slides german busin...  business   \n",
      "2       1101  bbc poll indicates economic gloom citizens in ...  business   \n",
      "3       1976  lifestyle  governs mobile choice  faster  bett...      tech   \n",
      "4        917  enron bosses in $168m payout eighteen former e...  business   \n",
      "\n",
      "   CategoryId  \n",
      "0           1  \n",
      "1           1  \n",
      "2           1  \n",
      "3           2  \n",
      "4           1  \n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "\n",
    "#Read data\n",
    "data=pd.read_csv('https://confrecordings.ams3.digitaloceanspaces.com/News.csv')\n",
    "\n",
    "#Count Unique values in Category column\n",
    "print(data['Category'].value_counts())\n",
    "\n",
    "#Create a dictionary d\n",
    "d={'sport':0 , 'business':1 ,'tech':2 ,'entertainment':3, 'politics':4}\n",
    "\n",
    "#Map dictionary to Category column\n",
    "data['CategoryId'] = data['Category'].map(d)\n",
    "print(data.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e14a60a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('popular', quiet=True)\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#Define a function for cleaning of text\n",
    "\n",
    "def clean_text(text):\n",
    "    text=text.lower()\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    text=' '.join(re.sub('[^a-zA-Z]',\" \",text).split())\n",
    "    text = nltk.word_tokenize(text)\n",
    "    text = \" \".join([wordnet_lemmatizer.lemmatize(word) for word in text if not word in stopwords.words('english')])\n",
    "    return text\n",
    "data['Text'] = data['Text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a61a82ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape =  (799, 5000)\n",
      "y.shape =  (799,)\n",
      "(559, 5000)\n",
      "(240, 5000)\n",
      "(559,)\n",
      "(240,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#store text column in x\n",
    "x = np.array(data.iloc[:,1].values)\n",
    "\n",
    "#store CategoryID column in y variable\n",
    "y = np.array(data.CategoryId.values)\n",
    "\n",
    "#Extract features using CountVectorizer\n",
    "cv = CountVectorizer(max_features = 5000)\n",
    "x = cv.fit_transform(data.Text).toarray()\n",
    "\n",
    "#print shape of x and y\n",
    "print(\"X.shape = \",x.shape)\n",
    "print(\"y.shape = \",y.shape)\n",
    "\n",
    "#Split data into training and testing dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 0, shuffle = True)\n",
    "\n",
    "#print shape of x_train , x_test, y_train , y_test\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62c3a055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.16666666666667\n",
      "Confusion matrix\n",
      "\n",
      " [[56  0  2  0  0]\n",
      " [ 1 47  3  0  1]\n",
      " [ 0  4 44  0  2]\n",
      " [ 2  1  2 39  0]\n",
      " [ 0  3  5  0 28]]\n"
     ]
    }
   ],
   "source": [
    "#Train model with SVM\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC()\n",
    "svm.fit(x_train,y_train)\n",
    "\n",
    "#Predict for x_test value\n",
    "y_pred = svm.predict(x_test)\n",
    "\n",
    "#Calculate accuracy of model\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test,y_pred)*100\n",
    "print(accuracy)\n",
    "\n",
    "#import library\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print('Confusion matrix\\n\\n', cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c728c594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sports News\n"
     ]
    }
   ],
   "source": [
    "#Predict for new value\n",
    "new_pred = cv.transform(['Hour ago, I contemplated retirement for a lot of reasons. I felt like people were not sensitive enough to my injuries. I felt like a lot of people were backed, why not me? I have done no less. I have won a lot of games for the team, and I am not feeling backed, said Ashwin']).toarray()\n",
    "\n",
    "yy = svm.predict(new_pred)\n",
    "\n",
    "#Return result array in form of text string again\n",
    "result = \"\"\n",
    "if yy == [0]:\n",
    "      result = \"Sports News\"\n",
    "elif yy == [1]:\n",
    "      result = \"Business News\"\n",
    "elif yy == [2]:\n",
    "      result = \"Tech News\"\n",
    "elif yy == [3]:\n",
    "      result = \"Entertainment News\"\n",
    "elif yy == [4]:\n",
    "      result = \"Politics News\"\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acd9057",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
